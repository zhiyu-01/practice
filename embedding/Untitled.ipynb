{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157dbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389218f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, max_length=50):\n",
    "    sentences = read_ptb()\n",
    "    vocab = Vocab(sentences, min_freq=10)\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "\n",
    "    class PTBDataset(paddle.io.Dataset):\n",
    "        def __init__(self, corpus):\n",
    "            # super().__init__()\n",
    "            self.data = np.zeros((len(corpus), max_length)).astype('int32')\n",
    "            for i, sentence in enumerate(corpus):\n",
    "                self.data[i, :len(sentence)] = sentence\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.data[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "    dataset = PTBDataset(corpus)\n",
    "\n",
    "    data_iter = paddle.io.DataLoader(dataset, batch_size=batch_size,shuffle=True)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f589141",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocab = load_data(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5429f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo_Model(nn.Layer):\n",
    "    def __init__(self, voc_size, output_size, emb_size=64, n_layers=2, lr=2e-3):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.emb = nn.Embedding(voc_size, emb_size, padding_idx=0, weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(0, 0.1)))\n",
    "        self.f_lstms = nn.LayerList([nn.LSTM(emb_size, output_size, time_major=False) if i==0 else nn.LSTM(output_size, output_size, time_major=False) for i in range(n_layers)])\n",
    "        self.b_lstms = nn.LayerList([nn.LSTM(emb_size, output_size, time_major=False) if i==0 else nn.LSTM(output_size, output_size, time_major=False) for i in range(n_layers)])\n",
    "        self.f_linear = nn.Linear(output_size, voc_size)\n",
    "        self.b_linear = nn.Linear(output_size, voc_size)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        emb = self.emb(seq)\n",
    "        f_outs = [emb[:, :-1, :]]\n",
    "        b_outs = [emb[:, 1:, :]]\n",
    "        for f_lstm in self.f_lstms:\n",
    "            f_out, (h_, c_) = f_lstm(f_outs[-1])\n",
    "            f_outs.append(f_out)\n",
    "            \n",
    "        for b_lstm in self.b_lstms:\n",
    "            b_out, (h_, c_) = b_lstm(b_outs[-1])\n",
    "            b_outs.append(b_out)\n",
    "        \n",
    "        f_out_l = self.f_linear(f_outs[-1])\n",
    "        b_out_l = self.b_linear(b_outs[-1])\n",
    "        return f_out_l, b_out_l\n",
    "    \n",
    "    def get_emb(self, seq):\n",
    "        fo, bo = self(seq)\n",
    "        embs = [paddle.concat([f[:, 1:, :], paddle.flip(b, axis=1)[:, :-1, :]], axis=2) for f, b in zip(fo, bo)]\n",
    "        for i, emb in enumerate(embs, start=1):\n",
    "            print('第{}的词向量维度为{}'.format(i, emb.shape[2]))\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c386ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "voc_size = len(vocab)\n",
    "output_size = 128\n",
    "batch_size = 64\n",
    "lr = 2e-3\n",
    "\n",
    "elmo = ELMo_Model(voc_size=voc_size, output_size=output_size)\n",
    "opt = paddle.optimizer.Adam(learning_rate=lr, parameters=elmo.parameters())\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579d2c22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Detect dataset only contains single fileds, return format changed since Paddle 2.1. In Paddle <= 2.0, DataLoader add a list surround output data(e.g. return [data]), and in Paddle >= 2.1, DataLoader return the single filed directly (e.g. return data). For example, in following code: \n",
      "\n",
      "import numpy as np\n",
      "from paddle.io import DataLoader, Dataset\n",
      "\n",
      "class RandomDataset(Dataset):\n",
      "    def __getitem__(self, idx):\n",
      "        data = np.random.random((2, 3)).astype('float32')\n",
      "\n",
      "        return data\n",
      "\n",
      "    def __len__(self):\n",
      "        return 10\n",
      "\n",
      "dataset = RandomDataset()\n",
      "loader = DataLoader(dataset, batch_size=1)\n",
      "data = next(loader())\n",
      "\n",
      "In Paddle <= 2.0, data is in format '[Tensor(shape=(1, 2, 3), dtype=float32)]', and in Paddle >= 2.1, data is in format 'Tensor(shape=(1, 2, 3), dtype=float32)'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:1, batch:658, loss:1.3957\n",
      "\n",
      "\n",
      "Epoch:2, batch:658, loss:1.4755\n",
      "\n",
      "\n",
      "Epoch:3, batch:658, loss:1.0948\n",
      "\n",
      "\n",
      "Epoch:4, batch:658, loss:1.1913\n",
      "\n",
      "\n",
      "Epoch:5, batch:658, loss:0.9537\n",
      "\n",
      "\n",
      "Epoch:6, batch:658, loss:0.8333\n",
      "\n",
      "\n",
      "Epoch:7, batch:658, loss:0.8670\n",
      "\n",
      "\n",
      "Epoch:8, batch:658, loss:0.7123\n",
      "\n",
      "\n",
      "Epoch:9, batch:658, loss:0.5120\n",
      "\n",
      "\n",
      "Epoch:10, batch:658, loss:0.6755\n",
      "\n",
      "\n",
      "Epoch:11, batch:658, loss:0.5826\n",
      "\n",
      "\n",
      "Epoch:12, batch:658, loss:0.7171\n",
      "\n",
      "\n",
      "Epoch:13, batch:658, loss:0.7279\n",
      "\n",
      "\n",
      "Epoch:14, batch:658, loss:0.5472\n",
      "\n",
      "\n",
      "Epoch:15, batch:658, loss:0.6205\n",
      "\n",
      "\n",
      "Epoch:16, batch:658, loss:0.6797\n",
      "\n",
      "\n",
      "Epoch:17, batch:658, loss:0.7520\n",
      "\n",
      "\n",
      "Epoch:18, batch:658, loss:0.5729\n",
      "\n",
      "\n",
      "Epoch:19, batch:658, loss:0.5044\n",
      "\n",
      "\n",
      "Epoch:20, batch:658, loss:0.6645\n",
      "\n",
      "\n",
      "Epoch:21, batch:658, loss:0.6159\n",
      "\n",
      "\n",
      "Epoch:22, batch:658, loss:0.4917\n",
      "\n",
      "\n",
      "Epoch:23, batch:658, loss:0.5226\n",
      "\n",
      "\n",
      "Epoch:24, batch:658, loss:0.5689\n",
      "\n",
      "\n",
      "Epoch:25, batch:658, loss:0.4974\n",
      "\n",
      "\n",
      "Epoch:26, batch:658, loss:0.5546\n",
      "\n",
      "\n",
      "Epoch:27, batch:658, loss:0.4441\n",
      "\n",
      "\n",
      "Epoch:28, batch:658, loss:0.5952\n",
      "\n",
      "\n",
      "Epoch:29, batch:658, loss:0.5166\n",
      "\n",
      "\n",
      "Epoch:30, batch:658, loss:0.4712\n",
      "\n",
      "\n",
      "Epoch:31, batch:658, loss:0.4003\n",
      "\n",
      "\n",
      "Epoch:32, batch:658, loss:0.5653\n",
      "\n",
      "\n",
      "Epoch:33, batch:658, loss:0.5377\n",
      "\n",
      "\n",
      "Epoch:34, batch:658, loss:0.5120\n",
      "\n",
      "\n",
      "Epoch:35, batch:658, loss:0.5164\n",
      "\n",
      "\n",
      "Epoch:36, batch:658, loss:0.5222\n",
      "\n",
      "\n",
      "Epoch:37, batch:658, loss:0.3896\n",
      "\n",
      "\n",
      "Epoch:38, batch:658, loss:0.4007\n",
      "\n",
      "\n",
      "Epoch:39, batch:658, loss:0.4228\n",
      "\n",
      "\n",
      "Epoch:40, batch:658, loss:0.3962\n",
      "\n",
      "\n",
      "Epoch:41, batch:658, loss:0.4350\n",
      "\n",
      "\n",
      "Epoch:42, batch:658, loss:0.3497\n",
      "\n",
      "\n",
      "Epoch:43, batch:658, loss:0.4356\n",
      "\n",
      "\n",
      "Epoch:44, batch:658, loss:0.3955\n",
      "\n",
      "\n",
      "Epoch:45, batch:658, loss:0.4229\n",
      "\n",
      "\n",
      "Epoch:46, batch:658, loss:0.4075\n",
      "\n",
      "\n",
      "Epoch:47, batch:658, loss:0.4713\n",
      "\n",
      "\n",
      "Epoch:48, batch:658, loss:0.3727\n",
      "\n",
      "\n",
      "Epoch:49, batch:658, loss:0.5156\n",
      "\n",
      "\n",
      "Epoch:50, batch:658, loss:0.3271\n",
      "\n",
      "\n",
      "Epoch:51, batch:658, loss:0.3916\n",
      "\n",
      "\n",
      "Epoch:52, batch:658, loss:0.3593\n",
      "\n",
      "\n",
      "Epoch:53, batch:658, loss:0.3963\n",
      "\n",
      "\n",
      "Epoch:54, batch:658, loss:0.3810\n",
      "\n",
      "\n",
      "Epoch:55, batch:658, loss:0.3700\n",
      "\n",
      "\n",
      "Epoch:56, batch:658, loss:0.4298\n",
      "\n",
      "\n",
      "Epoch:57, batch:658, loss:0.3795\n",
      "\n",
      "\n",
      "Epoch:58, batch:658, loss:0.4521\n",
      "\n",
      "\n",
      "Epoch:59, batch:658, loss:0.3622\n",
      "\n",
      "\n",
      "Epoch:60, batch:658, loss:0.4791\n",
      "\n",
      "\n",
      "Epoch:61, batch:658, loss:0.3367\n",
      "\n",
      "\n",
      "Epoch:62, batch:658, loss:0.4016\n",
      "\n",
      "\n",
      "Epoch:63, batch:658, loss:0.4563\n",
      "\n",
      "\n",
      "Epoch:64, batch:658, loss:0.5167\n",
      "\n",
      "\n",
      "Epoch:65, batch:658, loss:0.3303\n",
      "\n",
      "\n",
      "Epoch:66, batch:658, loss:0.4274\n",
      "\n",
      "\n",
      "Epoch:67, batch:658, loss:0.3954\n",
      "\n",
      "\n",
      "Epoch:68, batch:658, loss:0.3046\n",
      "\n",
      "\n",
      "Epoch:69, batch:658, loss:0.3631\n",
      "\n",
      "\n",
      "Epoch:70, batch:658, loss:0.3520\n",
      "\n",
      "\n",
      "Epoch:71, batch:658, loss:0.3989\n",
      "\n",
      "\n",
      "Epoch:72, batch:658, loss:0.4390\n",
      "\n",
      "\n",
      "Epoch:73, batch:658, loss:0.3854\n",
      "\n",
      "\n",
      "Epoch:74, batch:658, loss:0.3995\n",
      "\n",
      "\n",
      "Epoch:75, batch:658, loss:0.3553\n",
      "\n",
      "\n",
      "Epoch:76, batch:658, loss:0.3300\n",
      "\n",
      "\n",
      "Epoch:77, batch:658, loss:0.2515\n",
      "\n",
      "\n",
      "Epoch:78, batch:658, loss:0.4706\n",
      "\n",
      "\n",
      "Epoch:79, batch:658, loss:0.2850\n",
      "\n",
      "\n",
      "Epoch:80, batch:658, loss:0.3976\n",
      "\n",
      "\n",
      "Epoch:81, batch:658, loss:0.3056\n",
      "\n",
      "\n",
      "Epoch:82, batch:658, loss:0.3408\n",
      "\n",
      "\n",
      "Epoch:83, batch:658, loss:0.3908\n",
      "\n",
      "\n",
      "Epoch:84, batch:658, loss:0.2589\n",
      "\n",
      "\n",
      "Epoch:85, batch:658, loss:0.3183\n",
      "\n",
      "\n",
      "Epoch:86, batch:658, loss:0.3681\n",
      "\n",
      "\n",
      "Epoch:87, batch:658, loss:0.3147\n",
      "\n",
      "\n",
      "Epoch:88, batch:658, loss:0.3038\n",
      "\n",
      "\n",
      "Epoch:89, batch:658, loss:0.3982\n",
      "\n",
      "\n",
      "Epoch:90, batch:658, loss:0.3630\n",
      "\n",
      "\n",
      "Epoch:91, batch:658, loss:0.3991\n",
      "\n",
      "\n",
      "Epoch:92, batch:658, loss:0.3683\n",
      "\n",
      "\n",
      "Epoch:93, batch:658, loss:0.4222\n",
      "\n",
      "\n",
      "Epoch:94, batch:658, loss:0.2814\n",
      "\n",
      "\n",
      "Epoch:95, batch:658, loss:0.2648\n",
      "\n",
      "\n",
      "Epoch:96, batch:658, loss:0.3343\n",
      "\n",
      "\n",
      "Epoch:97, batch:658, loss:0.2772\n",
      "\n",
      "\n",
      "Epoch:98, batch:658, loss:0.3437\n",
      "\n",
      "\n",
      "Epoch:99, batch:658, loss:0.3096\n",
      "\n",
      "\n",
      "Epoch:100, batch:658, loss:0.3116\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    for batch, data in enumerate(data_iter()):\n",
    "        f_outs, b_outs = elmo(data)\n",
    "        \n",
    "        fo = paddle.reshape(f_outs, (-1, voc_size))\n",
    "        bo = paddle.reshape(b_outs, (-1, voc_size))\n",
    "        \n",
    "        f_label = paddle.reshape(data[:, 1:], (-1,))\n",
    "        b_label = paddle.reshape(data[:, :-1], (-1,))\n",
    "        f_label = paddle.cast(f_label, dtype='int64')\n",
    "        b_label = paddle.cast(b_label, dtype='int64')\n",
    "        \n",
    "        l = (loss(fo, f_label) + loss(bo, b_label)) / 2\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        \n",
    "        fo = f_outs[0].argmax(axis=1).numpy()\n",
    "        bo = b_outs[0].argmax(axis=1).numpy()\n",
    "\n",
    "    print('\\n\\nEpoch:{}, batch:{}, loss:{:.4f}'.format(i+1, batch+1, l.item()))\n",
    "paddle.save(elmo.state_dict(), 'elmo.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc44a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle24",
   "language": "python",
   "name": "paddle24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
