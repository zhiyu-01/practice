{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef1de76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://mofanpy.com/static/files/MRPC/msr_paraphrase_train.txt'\n",
    "test_url = 'https://mofanpy.com/static/files/MRPC/msr_paraphrase_test.txt'\n",
    "os.makedirs('./MRPC')\n",
    "for url in [train_url, test_url]:\n",
    "    r = requests.get(url)\n",
    "    with open('./MRPC/{}'.format(url.split('/')[-1]), 'w', encoding='utf-8') as f:\n",
    "        f.write(r.text.replace('<QUOTE>', '\"').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _text_standardize(text):\n",
    "    text = re.sub(\"\\d+(,\\d+)?(\\.\\d+)?\", \"<NUM>\", text)\n",
    "    text = re.sub(\"\\d+-+?\\d*\", \"<NUM>\", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea11a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 67: expected 5 fields, saw 6\n",
      "Skipping line 241: expected 5 fields, saw 6\n",
      "Skipping line 421: expected 5 fields, saw 6\n",
      "Skipping line 525: expected 5 fields, saw 6\n",
      "Skipping line 689: expected 5 fields, saw 6\n",
      "Skipping line 1391: expected 5 fields, saw 6\n",
      "Skipping line 1465: expected 5 fields, saw 6\n",
      "Skipping line 1693: expected 5 fields, saw 6\n",
      "Skipping line 2783: expected 5 fields, saw 6\n",
      "Skipping line 2933: expected 5 fields, saw 6\n",
      "Skipping line 3101: expected 5 fields, saw 6\n",
      "\n",
      "Skipping line 203: expected 5 fields, saw 6\n",
      "Skipping line 1311: expected 5 fields, saw 6\n",
      "Skipping line 1733: expected 5 fields, saw 6\n",
      "Skipping line 1759: expected 5 fields, saw 6\n",
      "Skipping line 1959: expected 5 fields, saw 6\n",
      "Skipping line 2877: expected 5 fields, saw 6\n",
      "Skipping line 2945: expected 5 fields, saw 6\n",
      "Skipping line 3643: expected 5 fields, saw 6\n",
      "Skipping line 3903: expected 5 fields, saw 6\n",
      "Skipping line 4017: expected 5 fields, saw 6\n",
      "Skipping line 4459: expected 5 fields, saw 6\n",
      "Skipping line 5011: expected 5 fields, saw 6\n",
      "Skipping line 5045: expected 5 fields, saw 6\n",
      "Skipping line 5617: expected 5 fields, saw 6\n",
      "Skipping line 5773: expected 5 fields, saw 6\n",
      "Skipping line 5839: expected 5 fields, saw 6\n",
      "Skipping line 5887: expected 5 fields, saw 6\n",
      "Skipping line 6481: expected 5 fields, saw 6\n",
      "Skipping line 6715: expected 5 fields, saw 6\n",
      "Skipping line 6917: expected 5 fields, saw 6\n",
      "Skipping line 6981: expected 5 fields, saw 6\n",
      "Skipping line 7285: expected 5 fields, saw 6\n",
      "Skipping line 7391: expected 5 fields, saw 6\n",
      "Skipping line 7909: expected 5 fields, saw 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('./MRPC')\n",
    "data = {'train':None, 'test':None}\n",
    "for f in files:\n",
    "    path = os.path.join('./MRPC', f)\n",
    "    df = pd.read_csv(path, sep='\\t', error_bad_lines=False)\n",
    "    label = df.iloc[:,0].values\n",
    "    s1 = df.iloc[:,3].values\n",
    "    s2 = df.iloc[:,4].values\n",
    "    if 'train' in f:\n",
    "        data['train'] = {'label':label, 's1':s1, 's2':s2}\n",
    "    else:\n",
    "        data['test'] = {'label':label, 's1':s1, 's2':s2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8495fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for n in ['train', 'test']:\n",
    "    for m in ['s1', 's2']:\n",
    "        for i in range(len(data[n][m])):\n",
    "            text = data[n][m][i]\n",
    "            text = str(text)\n",
    "            text = _text_standardize(text).split(' ')\n",
    "            vocab.update(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024b939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2i = {v:i for i,v in enumerate(vocab, start=1)}\n",
    "v2i['<PAD>'] = 0\n",
    "v2i['<UNK>'] = len(v2i)\n",
    "v2i['<SEP>'] = len(v2i)\n",
    "v2i['<GO>'] = len(v2i)\n",
    "i2v = {i:v for v,i in v2i.items()}\n",
    "\n",
    "for n in ['train', 'test']:\n",
    "    for m in ['s1', 's2']:\n",
    "        data[n][m+'id'] = [[v2i.get(v, v2i['<UNK>']) for v in str(s).split(' ')] for s in data[n][m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e6feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(paddle.io.Dataset):\n",
    "    def __init__(self, data, i2v, v2i, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.i2v = i2v\n",
    "        self.v2i = v2i\n",
    "        sequence = data['train']['s1id'] + data['train']['s2id']\n",
    "        sequence = [[v2i['<GO>']] + s + [v2i['<SEP>']] for s in sequence]\n",
    "        len_seq = len(sequence)\n",
    "        max_length = max([len(s) for s in sequence])\n",
    "        self.full_data = np.full((len_seq, max_length), fill_value=pad_id, dtype='int32')\n",
    "        \n",
    "        for i,s in enumerate(sequence):\n",
    "            l = len(s)\n",
    "            self.full_data[i, :l] = s\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.full_data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54725fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = Dataset(data, i2v, v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5324bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo_Model(nn.Layer):\n",
    "    def __init__(self, voc_size, output_size, emb_size=64, n_layers=2, lr=2e-3):\n",
    "        super().__init__()\n",
    "        self.voc_size = voc_size\n",
    "        self.emb = nn.Embedding(voc_size, emb_size, padding_idx=0, weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(0, 0.1)))\n",
    "        self.f_lstms = nn.LayerList([nn.LSTM(emb_size, output_size, time_major=False) if i==0 else nn.LSTM(output_size, output_size, time_major=False) for i in range(n_layers)])\n",
    "        self.b_lstms = nn.LayerList([nn.LSTM(emb_size, output_size, time_major=False) if i==0 else nn.LSTM(output_size, output_size, time_major=False) for i in range(n_layers)])\n",
    "        self.f_linear = nn.Linear(output_size, voc_size)\n",
    "        self.b_linear = nn.Linear(output_size, voc_size)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        emb = self.emb(seq)\n",
    "        f_outs = [emb[:, :-1, :]]\n",
    "        b_outs = [paddle.flip(emb[:, 1:, :], axis=1)]\n",
    "        for f_lstm in self.f_lstms:\n",
    "            f_out, (h_, c_) = f_lstm(f_outs[-1])\n",
    "            f_outs.append(f_out)\n",
    "            \n",
    "        for b_lstm in self.b_lstms:\n",
    "            b_out, (h_, c_) = b_lstm(b_outs[-1])\n",
    "            b_outs.append(b_out)\n",
    "        \n",
    "        f_out_l = self.f_linear(f_outs[-1])\n",
    "        b_out_l = self.b_linear(b_outs[-1])\n",
    "        return f_out_l, b_out_l\n",
    "    \n",
    "    def get_emb(self, seq):\n",
    "        fo, bo = self(seq)\n",
    "        embs = [paddle.concat([f[:, 1:, :], paddle.flip(b, axis=1)[:, :-1, :]], axis=2) for f, b in zip(fo, bo)]\n",
    "        for i, emb in enumerate(embs, start=1):\n",
    "            print('第{}的词向量维度为{}'.format(i, emb.shape[2]))\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bdc6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "voc_size = len(v2i)\n",
    "output_size = 128\n",
    "batch_size = 64\n",
    "lr = 2e-3\n",
    "\n",
    "dataloader = paddle.io.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "elmo = ELMo_Model(voc_size=voc_size, output_size=output_size)\n",
    "opt = paddle.optimizer.Adam(learning_rate=lr, parameters=elmo.parameters())\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdbe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Detect dataset only contains single fileds, return format changed since Paddle 2.1. In Paddle <= 2.0, DataLoader add a list surround output data(e.g. return [data]), and in Paddle >= 2.1, DataLoader return the single filed directly (e.g. return data). For example, in following code: \n",
      "\n",
      "import numpy as np\n",
      "from paddle.io import DataLoader, Dataset\n",
      "\n",
      "class RandomDataset(Dataset):\n",
      "    def __getitem__(self, idx):\n",
      "        data = np.random.random((2, 3)).astype('float32')\n",
      "\n",
      "        return data\n",
      "\n",
      "    def __len__(self):\n",
      "        return 10\n",
      "\n",
      "dataset = RandomDataset()\n",
      "loader = DataLoader(dataset, batch_size=1)\n",
      "data = next(loader())\n",
      "\n",
      "In Paddle <= 2.0, data is in format '[Tensor(shape=(1, 2, 3), dtype=float32)]', and in Paddle >= 2.1, data is in format 'Tensor(shape=(1, 2, 3), dtype=float32)'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    for batch, data in enumerate(dataloader()):\n",
    "        f_outs, b_outs = elmo(data)\n",
    "        \n",
    "        fo = paddle.reshape(f_outs, (-1, voc_size))\n",
    "        bo = paddle.reshape(b_outs, (-1, voc_size))\n",
    "        \n",
    "        f_label = paddle.reshape(data[:, 1:], (-1,))\n",
    "        b_label = paddle.reshape(paddle.flip(data[:, :-1], axis=1), (-1))\n",
    "        f_label = paddle.cast(f_label, dtype='int64')\n",
    "        b_label = paddle.cast(b_label, dtype='int64')\n",
    "        \n",
    "        l = (loss(fo, f_label) + loss(bo, b_label)) / 2\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        \n",
    "        fo = f_outs[0].zrgmax(axis=1).numpy()\n",
    "        bo = paddle.flip(b_outs, axis=1)[0].argmax(axis=1).numpy()\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('\\n\\nEpoch:{}, batch:{}, loss:{:.4f}'.format(i+1, batch+1, loss.item()),\n",
    "                '\\ntarget:{}'.format(' '.join([i2v[j] for j in data[0].numpy() if j != train_data.pad_id])),\n",
    "                '\\nforward:{}'. format(' '.join([i2v[j] for j in fo if j != train_data.pad_id])),\n",
    "                '\\nbackward:{}'. format(' '.join([i2v[j] for j in bo if j != train_data.pad_id])),\n",
    "                )\n",
    "        paddle.save(elmo.state_dict(), './work/elmo_{}.pdparams'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322daac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle24",
   "language": "python",
   "name": "paddle24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
